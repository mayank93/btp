\chapter{Introduction}
\label{Chapter1}
\lhead{Chapter 1. \emph{Introduction}}

Clustering is a Data Mining technique, that, given a dataset, divides it into
groups of objects such that the objects in a group will be
similar to one another and different from the objects in other
groups\cite{bib17}. The objects are represented as a point, where
each dimension corresponds to an attribute/feature and the
feature value of each object determines its coefficient in the
corresponding dimension. The data points are grouped into
clusters based on the some similarity metric. The clustering
algorithms have large number of applications such as marketing (e.g., customer segmentation), image analysis, bio-
informatics, document classification, indexing, etc.

\section{Issues with Clustering Techniques}

Most clustering algorithms do not work efficiently in higher
dimensional space because of the inherent sparsity of the
data \cite{bib3}. In high dimensional space, it is likely that the
distance of any two points is almost the same for a large
class of common distributions \cite{bib5}. So, a clustering algorithm
employ feature selection based approach know as subspaces
clustering \cite{bib11}. The goal of the subspace clustering is to
find the particular dimensions on which the data points are
correlated and pruning away the remaining dimensions that
reduces the noise in the data.

The problem in these algorithms is that picking certain dimensions in advance can
lead to a loss of information. Furthermore, in many real
data, some points are correlated with respect to a given set
of dimensions and others are correlated with respect to different dimensions. Thus, it may not always be feasible to
prune off too many dimensions without considering the data
at the same time incurring a substantial loss of information.
Alternatively, the effects of dimensionality can be reduced
by a dimensionality reduction technique\cite{bib6}, however, in-
formation from all dimensions is uniformly transformed and
relevant information for some clusters may be reduced. Also,
the clusters may be hard to understand.

\section{Available Algorithms}

The projected clustering\cite{bib1} alogrithms overcome some of the
issues of the subspace clustering. In projected clustering, a
set of data points with an associated set of relevant dimensions are employed such that the data points are similar to
each other in the subspace formed by the relevant dimensions, but dissimilar to data point outside the cluster. The
widely used distance measures are more meaningful in projections of the high-dimensional space, where the object
values are dense\cite{bib9}. In other words, it is more likely for the data
to form dense, meaningful clusters in a high-dimensional
subspace. CLIQUE \cite{bib3}, PROCLUS \cite{bib1}, ORCLUS \cite{bib2}, and DOC \cite{bib12} are some efforts in the projected clustering approaches.

These approaches suffer from the quality of the clusters and
consumes lot of time. An effort is made to exploit the frequent pattern mining in the area of projected clustering to
address the issue of quality clusters\cite{bib18, bib19}. However, the
frequent pattern mining\cite{bib4} approach identifies the itemsets
(features/attributes) with high support. In frequent pattern
based projected clustering approach, we observed that only
frequent itemsets are considered and the infrequent itemsets
are completely eliminated from the clustering process.

\section{Our Approach}

We propose an approach to refine the clusters
generated by the frequent pattern based projected clustering approach and produce more qualitative clusters. We
observed that a high support pattern includes few items
and eliminates non-frequent items.
As a result of elimination of some of the items, the influence of other items
is not considered in the process of clustering. Due to elimination of some
data values, the the quality of clusters gets affected. In the
proposed approach, for a given cluster, we identify the mem-
bership of each data point employing the notion called diversity. The diversity identify the non-members of the given
cluster and remove the objects from the cluster. We conduct
the experiment on the real world datasets and show that the
proposed approach improves the quality of clusters.
